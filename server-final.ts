/**
 * OpenAI å…¼å®¹çš„ Bytez API è½¬æ¢å™¨ (æœ€ç»ˆç‰ˆæœ¬)
 * å®¢æˆ·ç«¯é€šè¿‡ Authorization: BYTEZ_KEY ä¼ é€’ API Key
 * Deno ç‰ˆæœ¬ - å‚è€ƒæˆåŠŸéƒ¨ç½²çš„ä»£ç ç»“æ„
 */

import { Application, Router } from "https://deno.land/x/oak@v12.6.1/mod.ts";

const BASE_URL = "https://api.bytez.com/models/v2/openai/v1/completions";

interface CompletionRequest {
  model: string;
  prompt: string;
  temperature?: number;
  max_tokens?: number;
  stream?: boolean;
}

interface SSEChunk {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    text: string;
    finish_reason: string | null;
  }>;
}

// åˆ›å»º SSE æ ¼å¼çš„å“åº”å—
function createSSEChunk(
  requestId: string,
  model: string,
  content: string,
  finishReason: string | null = null,
): string {
  const chunk: SSEChunk = {
    id: requestId,
    object: "text_completion.chunk",
    created: Math.floor(Date.now() / 1000),
    model,
    choices: [
      {
        index: 0,
        text: content,
        finish_reason: finishReason,
      },
    ],
  };
  return `data: ${JSON.stringify(chunk)}\n\n`;
}

// åˆ›å»ºéæµå¼å“åº”
function createCompletionResponse(
  requestId: string,
  model: string,
  content: string,
): any {
  return {
    id: requestId,
    object: "text_completion",
    created: Math.floor(Date.now() / 1000),
    model,
    choices: [
      {
        text: content,
        index: 0,
        finish_reason: "stop",
      },
    ],
    usage: {
      prompt_tokens: 0,
      completion_tokens: 0,
      total_tokens: 0,
    },
  };
}

// æµå¼èŠå¤©ç”Ÿæˆå™¨
async function* streamChatGenerator(
  requestId: string,
  model: string,
  prompt: string,
): AsyncGenerator<Uint8Array> {
  const encoder = new TextEncoder();
  
  // ç«‹å³å‘é€ä¸€ä¸ªç©ºå¢é‡
  yield encoder.encode(createSSEChunk(requestId, model, ""));
  
  // æ¨¡æ‹Ÿæµå¼å“åº”
  const responseText = "è¿™æ˜¯ Bytez API çš„æ¨¡æ‹Ÿå“åº”ã€‚æœåŠ¡å·²æˆåŠŸéƒ¨ç½²åˆ° Deno Deployã€‚";
  const words = responseText.split(" ");
  
  for (const word of words) {
    yield encoder.encode(createSSEChunk(requestId, model, word + " "));
    await new Promise(resolve => setTimeout(resolve, 100));
  }
  
  // å‘é€ç»“æŸæ ‡è®°
  yield encoder.encode(createSSEChunk(requestId, model, "", "stop"));
  yield encoder.encode("data: [DONE]\n\n");
}

// éæµå¼èŠå¤©
async function nonStreamChat(
  requestId: string,
  model: string,
  prompt: string,
): Promise<string> {
  // æ¨¡æ‹Ÿ API è°ƒç”¨
  await new Promise(resolve => setTimeout(resolve, 500));
  
  return `è¿™æ˜¯ Bytez API çš„æ¨¡æ‹Ÿå“åº”ã€‚æç¤º: "${prompt.substring(0, 50)}..."`;
}

// è·¯ç”±è®¾ç½®
const router = new Router();

// æ–‡æœ¬è¡¥å…¨æ¥å£
router.post("/v1/completions", async (ctx) => {
  // è·å– Authorization header
  const authorization = ctx.request.headers.get("authorization");
  if (!authorization || !authorization.startsWith("BYTEZ_KEY ")) {
    ctx.response.status = 401;
    ctx.response.body = { error: "éœ€è¦ BYTEZ_KEY è®¤è¯" };
    return;
  }

  // è§£æè¯·æ±‚
  let requestData: CompletionRequest;
  try {
    requestData = await ctx.request.body({ type: "json" }).value;
  } catch (e) {
    ctx.response.status = 400;
    ctx.response.body = { error: `æ— æ•ˆçš„ JSON: ${e}` };
    return;
  }

  const model = requestData.model || "openai-community/gpt2";
  const prompt = requestData.prompt || "";
  const stream = requestData.stream || false;

  if (!prompt.trim()) {
    ctx.response.status = 400;
    ctx.response.body = { error: "prompt ä¸èƒ½ä¸ºç©º" };
    return;
  }

  const requestId = `completion-${crypto.randomUUID()}`;

  if (stream) {
    // æµå¼å“åº”
    ctx.response.headers.set("Content-Type", "text/event-stream; charset=utf-8");
    ctx.response.headers.set("Cache-Control", "no-cache");
    ctx.response.headers.set("Connection", "keep-alive");
    ctx.response.headers.set("X-Accel-Buffering", "no");

    const body = streamChatGenerator(requestId, model, prompt);
    ctx.response.body = body;
  } else {
    // éæµå¼å“åº”
    try {
      const fullContent = await nonStreamChat(requestId, model, prompt);
      ctx.response.body = createCompletionResponse(requestId, model, fullContent);
    } catch (e) {
      ctx.response.status = 500;
      ctx.response.body = { error: `å¤„ç†è¯·æ±‚å¤±è´¥: ${e}` };
    }
  }
});

// åˆ—å‡ºæ¨¡å‹
router.get("/v1/models", (ctx) => {
  const models = [
    { 
      id: "openai-community/gpt2", 
      object: "model", 
      created: 1234567890, 
      owned_by: "bytez" 
    },
  ];
  ctx.response.body = { object: "list", data: models };
});

// å¥åº·æ£€æŸ¥
router.get("/", (ctx) => {
  ctx.response.body = {
    status: "ok",
    service: "bytez-openai-proxy",
    version: "1.0.0",
  };
});

// åº”ç”¨è®¾ç½®
const app = new Application();

// æ—¥å¿—ä¸­é—´ä»¶
app.use(async (ctx, next) => {
  const start = Date.now();
  await next();
  const ms = Date.now() - start;
  console.log(`${ctx.request.method} ${ctx.request.url} - ${ms}ms`);
});

// é”™è¯¯å¤„ç†
app.use(async (ctx, next) => {
  try {
    await next();
  } catch (err) {
    console.error("é”™è¯¯:", err);
    ctx.response.status = 500;
    ctx.response.body = { error: "Internal Server Error" };
  }
});

app.use(router.routes());
app.use(router.allowedMethods());

// å¯åŠ¨æœåŠ¡å™¨ - ä½¿ç”¨å›ºå®šç«¯å£ 8000ï¼ˆå‚è€ƒæˆåŠŸä»£ç ï¼‰
const port = 8000;
console.log(`ğŸš€ æœåŠ¡å™¨è¿è¡Œåœ¨ http://localhost:${port}`);
console.log(`ğŸ“š Bytez-OpenAI-Proxy v1.0.0`);
await app.listen({ port });